import json
import numpy as np
import os
import pandas as pd

from comchoice.aggregate import borda, divisiveness, win_rate
from comchoice.preprocessing.to_pairwise import to_pairwise
from glob import glob
from scipy.stats import kurtosis, skew
from epitools import within_p, between_p

RATE_THRESHOLD = 0.03
VOTES_POLLING = 0
# Values accepted are {"rate": "rank"}
_labels = open("acronyms.json", encoding="utf-8")
_labels = json.load(_labels)

"""
Read CSV files generated by each pipeline to generate data to use in regressions
and descriptive analysis througout the manuscript.

Data origin, features and data cleaning procedure is explained in the supplementary material.
"""

regression = False


def er_polarization(rates, weights, alpha=0.25, K=1000):
    """Esteban and Ray Polarization (1994)

    Parameters
    ----------
    rates : _type_
        _description_
    weights : _type_
        _description_
    alpha : float, optional
        _description_, by default 0.25
    K : int, optional
        _description_, by default 1000

    Returns
    -------
    _type_
        _description_
    """
    xx = np.multiply.outer(weights ** (1 + alpha), weights)
    yy = np.absolute(np.subtract.outer(rates, rates))
    # avg = np.average(rates, weights=weights)

    return K * np.sum(xx * yy)


def calculate_divisiveness(
    year,
    country,
    location_level,
    election_round,
    method="std"
):
    # labels = _labels[f"{country}_{year}"]
    runoff_countries = ["France", "Chile", "Brazil", "Romania"]

    df = pd.read_csv(
        f"data_output/{country}/{year}_{election_round}.csv.gz",
        compression="gzip"
    )
    df.columns = [x.lower() for x in df.columns]
    df = df[~df["candidate"].isin(["OTHER"])]
    # dd = df.groupby("candidate").agg({"value": "sum"})
    # dd["rate"] = dd.apply(lambda x: x/x.sum())
    # values = list(dd[dd["rate"] > RATE_THRESHOLD].index.unique())

    if country in runoff_countries:
        df_runoff = pd.read_csv(
            f"data_output/{country}/{year}_runoff.csv.gz", compression="gzip")
        df_runoff.columns = [x.lower() for x in df_runoff.columns]

        df_runoff = df_runoff[df_runoff["candidate"].isin(values)]

    df_location = pd.read_csv(
        f"data_output/{country}/{year}_{election_round}_location.csv.gz", compression="gzip")

    # df = df[df["candidate"].isin(values)].copy()

    # ee = df.groupby("polling_id").agg({"value": "sum"})
    # values_polling = list(ee[ee["value"] > VOTES_POLLING].index.unique())
    # df = df[df["polling_id"].isin(values_polling)].copy()

    # # Re-calculates voting percentage of candidates after removing outliers
    # tt = df.groupby(["polling_id", "candidate"]).agg({"value": "sum"})
    # tt["rate"] = tt.groupby(level=[0]).apply(lambda x: x/x.sum())
    # tt = tt.reset_index()
    # tt["rank"] = tt.groupby(["polling_id"])["value"].rank(
    #     "min", ascending=False).astype(int)
    # df = tt[["polling_id", "candidate", "rate", "rank", "value"]]

    # df1 = pd.merge(df, df_location[["polling_id", location_level]])

    # df1 = df1.groupby([location_level, "candidate"]).agg({"value": "sum"})
    # df1["rate"] = df1.groupby(level=[0]).apply(lambda x: x/x.sum()).fillna(0)
    # df1 = df1.reset_index()

    # if country in runoff_countries:
    #     df2 = pd.merge(df_runoff, df_location[["polling_id", location_level]])

    #     df2 = df2.groupby([location_level, "candidate"]).agg({"value": "sum"})
    #     df2["rate"] = df2.groupby(
    #         level=[0], group_keys=False).apply(lambda x: x/x.sum())
    #     df2 = df2.reset_index()

    #     df_rounds = pd.merge(df1, df2, on=[location_level, "candidate"])
    #     df_rounds["diff"] = df_rounds["rate_y"] - df_rounds["rate_x"]

    path = f"data_output/{country}/{year}_pairwise.csv.gz"

    if not os.path.isfile(path):

        df_pwc = to_pairwise(
            df,
            alternative="candidate",
            verbose=True,
            voter="polling_id"
        )
        df_pwc.to_csv(path, compression="gzip", index=False)

    else:
        df_pwc = pd.read_csv(path, compression="gzip")

    data = pd.merge(df_location[[location_level, "polling_id"]].drop_duplicates(
    ), df.copy(), on="polling_id").copy()

    if method in ["std", "std_rank"]:
        _ = "rate" if method == "std" else "rank"
        # Uses Standard deviation of a feature to measure how divisive a location is.
        df_dv = data.groupby([location_level, "candidate"])\
            .agg({_: "std"})\
            .rename(columns={_: "value"}).reset_index()

    elif method == "divisiveness":
        output = []

        data = pd.merge(df_location[[location_level, "polling_id"]].drop_duplicates(
        ), df_pwc.copy(), on="polling_id").copy()
        for i, tmp in data.groupby(location_level):
            print(i)
            dv = divisiveness(
                tmp,
                method=win_rate,
                voter="polling_id",
                method_kws=dict(voter="polling_id")
            )
            dv[location_level] = i
            output.append(dv)

        df_dv = pd.concat(output, ignore_index=True)
        df_dv = df_dv.rename(columns={"alternative": "candidate"})

    elif method in ["kurtosis", "skew"]:

        output = []
        _ = skew if method == "skew" else kurtosis

        for i, tmp in data.groupby(location_level):
            for candidate, tmp_cand in tmp.groupby("candidate"):
                output.append({
                    "candidate": candidate,
                    "value": _(tmp_cand["rate"].dropna()),
                    location_level: i
                })

        df_dv = pd.DataFrame(output)

    elif method == "er":
        output = []
        alpha = 0.25
        df_tmp = pd.merge(
            data,
            data.groupby("polling_id").agg({"value": "sum"}).rename(
                columns={"value": "N"}).reset_index(),
            on="polling_id"
        )
        for idx, _data in df_tmp.groupby(["candidate", location_level]):
            candidate, geography = idx
            _data = _data.fillna(0)
            _data["weight"] = _data["N"] / _data["N"].sum()
            weights = _data["weight"].fillna(0).values
            rates = _data["rate"].fillna(0).values

            value = er_polarization(rates, weights, alpha=alpha, K=1000)

            output.append({
                "value": value,
                "candidate": candidate,
                location_level: geography
            })

        df_dv = pd.DataFrame(output)

    elif method == "wstd":
        def weighted_sd(input_df):
            weights = input_df["value"] / input_df["value"].sum()
            vals = input_df["rate"]
            weighted_avg = np.average(vals, weights=weights)
            numer = np.sum(weights * (vals - weighted_avg)**2)
            denom = ((vals.count()-1)/vals.count())*np.sum(weights)
            return np.sqrt(numer/denom)

        _ = "rate"
        # Uses  Weighted Standard deviation of a feature to measure how divisive a location is.
        df_dv = data.groupby([location_level, "candidate"])\
            .apply(weighted_sd)\
            .reset_index()\
            .rename(columns={0: "value"})

    elif method == "nv":

        nv_output = []
        for idx, dt in data.groupby(location_level):
            geography = idx
            print(geography)

            dd = dt.groupby("candidate").agg({"value": "sum"})
            dd["rate"] = dd.apply(lambda x: x/x.sum())
            values = list(dd[dd["rate"] > RATE_THRESHOLD].index.unique())

            dt = dt[dt["candidate"].isin(values)].copy()

            ee = dt.groupby("polling_id").agg({"value": "sum"})
            values_polling = list(
                ee[ee["value"] > VOTES_POLLING].index.unique())
            dt = dt[dt["polling_id"].isin(values_polling)].copy()

            # Re-calculates voting percentage of candidates after removing outliers
            tt = dt.groupby(["polling_id", "candidate"]).agg({"value": "sum"})
            tt["rate"] = tt.groupby(
                level=[0], group_keys=False).apply(lambda x: x/x.sum())
            tt = tt.reset_index()
            tt["rank"] = tt.groupby(["polling_id"])["value"].rank(
                "min", ascending=False).astype(int)
            dt = tt[["polling_id", "candidate", "rate", "rank", "value"]]

            dt = pd.merge(
                dt, df_location[["polling_id", location_level]]).drop_duplicates()
            dt = dt.reset_index(drop=True)

            df_between = between_p(dt)
            df_between["type"] = "Between"

            df_within = within_p(dt)
            df_within["type"] = "Within"

            df_polarization = pd.concat([df_between, df_within])

            df_polarization[location_level] = geography

            nv_output.append(df_polarization)

        df_dv = pd.concat(nv_output, ignore_index=True)

    path = f"data_output/{country}/{year}_divisiveness_{location_level}_{method}_{election_round}.csv.gz"
    df_dv.to_csv(path, compression="gzip", index=False)

    # location = "polling_id"

    # def get_rate(df, location=["polling_id"], level=[0]):
    #     df_tmp = df.groupby(location + ["candidate"]).agg({"value": "sum"})
    #     df_tmp["rate"] = df_tmp.groupby(level=level).apply(lambda x: x/x.sum())
    #     df_tmp = df_tmp.reset_index()

    #     return df_tmp

    # if regression:
    #     df = pd.merge(df, df_location, on="polling_id")

    #     df_a = get_rate(
    #         df, location=[location_level, "polling_id"], level=[0, 1])
    #     df_a = pd.merge(df_a, df_dv.rename(columns={"value": "divisiveness"}), on=[
    #                     location_level, "candidate"])

    #     if country in runoff_countries:
    #         df_b = get_rate(df_runoff)

    #     dd = df_a.pivot_table(
    #         index=["polling_id"],
    #         columns=["candidate"],
    #         values=["rate", "value", "divisiveness"]
    #     ).reset_index()

    #     cols = []
    #     for column in dd.columns:
    #         a, b = column
    #         new_column = None
    #         if not b:
    #             new_column = a
    #         else:
    #             new_column = f"{a}_{b}"  # labels

    #         cols.append(new_column)

    #     dd.columns = cols
    #     if country in runoff_countries:
    #         dd = pd.merge(dd, df_b, on="polling_id")
    #     encoding = "utf-8"
    #     if country == "Romania":
    #         encoding = "iso8859_16"
    #     dd.to_csv(
    #         f"data_regressions/{country}_{year}_polling_station_{method}.csv",
    #         encoding=encoding,
    #         index=False
    #     )


for year, country, election_round, location_level in [
    # (2013, "Chile", "runoff", "region_id"),
    # (2017, "Chile", "runoff", "region_id"),
    # (2021, "Chile", "runoff", "region_id"),
    # (2013, "Chile", "first_round", "region_id"),
    # (2017, "Chile", "first_round", "region_id"),
    # (2021, "Chile", "first_round", "region_id"),
    # (2000, "United States", "first_round", "state"),
    # (2004, "United States", "first_round", "state"),
    # (2008, "United States", "first_round", "state"),
    # (2012, "United States", "first_round", "state"),
    # (2016, "United States", "first_round", "state"),
    (2016, "United States", "senate", "state"),
    # # (2009, "Romania", "county_name"),
    # # (2017, "Chile", "province"),
    # # (2021, "Chile", "province"),
    # # (2017, "Chile", "commune"),
    # # (2021, "Chile", "commune"),
    # (2002, "France", "runoff", "department_id"),
    # (2007, "France", "runoff", "department_id"),
    # (2012, "France", "runoff", "department_id"),
    # (2017, "France", "runoff", "department_id"),
    # (2022, "France", "runoff", "department_id"),
    # (2002, "France", "first_round", "department_id"),
    # (2007, "France", "first_round", "department_id"),
    # (2012, "France", "first_round", "department_id"),
    # (2017, "France", "first_round", "department_id"),
    # (2022, "France", "first_round", "department_id"),
    # (2018, "Brazil", "runoff", "region_id")
    # (2022, "Italy", "first_round", "province_id"),
    # (2021, "Germany", "first_round", "constituency"),
    # (2019, "Spain", "first_round", "province_id"),
    # (2021, "Germany", "nuts_2"),
    # (2021, "Germany", "nuts_3"),
]:
    # "er" "divisiveness" "std", "std_rank", "skew", "kurtosis"
    for method in ["nv"]:
        print(year, country, location_level, election_round, method)
        calculate_divisiveness(
            year, country, location_level, election_round, method)
