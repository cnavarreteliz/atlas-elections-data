import json
import numpy as np
import os
import pandas as pd
import argparse

from comchoice.aggregate import borda, divisiveness, win_rate
from comchoice.preprocessing.to_pairwise import to_pairwise
from glob import glob
from scipy.stats import kurtosis, skew
from epitools import within_p, between_p

RATE_THRESHOLD = 0.03
VOTES_POLLING = 0
# Values accepted are {"rate": "rank"}
_labels = open("acronyms.json", encoding="utf-8")
_labels = json.load(_labels)

# SELET DATASET TO USE
parser = argparse.ArgumentParser()
# year, country, location_level, election_round, method
parser.add_argument("-y", "--year", default=2022, type=int, required=True)
parser.add_argument("-c", "--country", default="France",
                    type=str, required=True)
parser.add_argument("-l", "--level", default="department_id",
                    type=str, required=True)
parser.add_argument("-r", "--round", default="first_round",
                    type=str, required=False)
parser.add_argument("-m", "--method", default="nv",
                    type=str, required=False)

parser.add_argument("-n", "--ncandidates", default=2,
                    type=int, required=False)

parser.add_argument("-a", "--alpha", default=0.25,
                    type=float, required=False)

parser.add_argument("-g", "--gamma", default=0.5,
                    type=float, required=False)

parser.add_argument("-f", "--filter", default=1,
                    type=int, required=False)

args = parser.parse_args()

year = args.year
country = args.country
location_level = args.level
election_round = args.round
method = args.method
n_candidates = args.ncandidates
ER_alpha = args.alpha
TW_gamma = args.gamma
flag_candidates = args.filter

"""
Read CSV files generated by each pipeline to generate data to use in regressions
and descriptive analysis througout the manuscript.

Data origin, features and data cleaning procedure is explained in the supplementary material.
"""

regression = False


def er_polarization(
    input_df,
    alpha=0
):
    """Esteban and Ray Polarization (1994)
    """

    weights = input_df["value"].values
    rates = input_df["rate"].values
    K = 1 / (weights.sum() ** (2 + alpha))

    xx = np.multiply.outer(weights ** (1 + alpha), weights)
    yy = np.absolute(np.subtract.outer(rates, rates))
    # avg = np.average(rates, weights=weights)

    return K * np.sum(xx * yy)


def tsui_wang(
    input_df,
    K: int = 1,
    gamma: float = 0.5
):
    median = np.median(input_df["rate"])
    population = np.sum(input_df["value"])

    value = 0
    for i, xx in input_df.iterrows():
        value += xx.value * np.absolute((xx.rate - median)/median) ** gamma

    return K * value / population


def calculate_divisiveness(
    year,
    country,
    location_level,
    election_round,
    method="std"
):
    # labels = _labels[f"{country}_{year}"]
    runoff_countries = ["France", "Chile", "Brazil", "Romania"]

    df = pd.read_csv(
        f"data_output/{country}/{year}_{election_round}.csv.gz",
        compression="gzip"
    )
    df.columns = [x.lower() for x in df.columns]
    df = df[~df["candidate"].isin(["OTHER"])]
    if flag_candidates == 1 and "flag_candidates" in list(df):
        df = df[df["flag_candidates"] == 1].copy()

    # dd = df.groupby("candidate").agg({"value": "sum"})
    # dd["rate"] = dd.apply(lambda x: x/x.sum())
    # values = list(dd[dd["rate"] > RATE_THRESHOLD].index.unique())

    # if country in runoff_countries:
    #     df_runoff = pd.read_csv(
    #         f"data_output/{country}/{year}_runoff.csv.gz", compression="gzip")
    #     df_runoff.columns = [x.lower() for x in df_runoff.columns]

    #     df_runoff = df_runoff[df_runoff["candidate"].isin(values)]

    df_location = pd.read_csv(
        f"data_output/{country}/{year}_{election_round}_location.csv.gz", compression="gzip")

    path = f"data_output/{country}/{year}_pairwise.csv.gz"

    if not os.path.isfile(path):

        df_pwc = to_pairwise(
            df,
            alternative="candidate",
            verbose=True,
            voter="polling_id"
        )
        df_pwc.to_csv(path, compression="gzip", index=False)

    else:
        df_pwc = pd.read_csv(path, compression="gzip")

    data = pd.merge(df_location[[location_level, "polling_id"]].drop_duplicates(
    ), df.copy(), on="polling_id", how="right").copy()

    if method in ["std", "std_rank"]:
        _ = "rate" if method == "std" else "rank"
        # Uses Standard deviation of a feature to measure how divisive a location is.
        df_dv = data.groupby([location_level, "candidate"])\
            .agg({_: "std"})\
            .rename(columns={_: "value"}).reset_index()

    elif method == "divisiveness":
        output = []

        data = pd.merge(df_location[[location_level, "polling_id"]].drop_duplicates(
        ), df_pwc.copy(), on="polling_id").copy()
        for i, tmp in data.groupby(location_level):
            print(i)
            dv = divisiveness(
                tmp,
                method=win_rate,
                voter="polling_id",
                method_kws=dict(voter="polling_id")
            )
            dv[location_level] = i
            output.append(dv)

        df_dv = pd.concat(output, ignore_index=True)
        df_dv = df_dv.rename(columns={"alternative": "candidate"})

    elif method in ["kurtosis", "skew"]:

        output = []
        _ = skew if method == "skew" else kurtosis

        for i, tmp in data.groupby(location_level):
            for candidate, tmp_cand in tmp.groupby("candidate"):
                output.append({
                    "candidate": candidate,
                    "value": _(tmp_cand["rate"].dropna()),
                    location_level: i
                })

        df_dv = pd.DataFrame(output)

    elif method == "er":
        output = []
        alpha = ER_alpha
        df_tmp = pd.merge(
            data,
            data.groupby("polling_id").agg({"value": "sum"}).rename(
                columns={"value": "N"}).reset_index(),
            on="polling_id"
        )
        for idx, _data in df_tmp.groupby(["candidate", location_level]):
            candidate, geography = idx
            _data = _data.fillna(0)
            # _data["weight"] = _data["N"] / _data["N"].sum()

            value = er_polarization(_data, alpha=alpha)

            output.append({
                "value": value,
                "candidate": candidate,
                location_level: geography
            })

        df_dv = pd.DataFrame(output)

    elif method == "tw":
        output = []
        gamma = TW_gamma
        df_tmp = pd.merge(
            data,
            data.groupby("polling_id").agg({"value": "sum"}).rename(
                columns={"value": "N"}).reset_index(),
            on="polling_id"
        )
        for idx, _data in df_tmp.groupby(["candidate", location_level]):
            candidate, geography = idx
            _data = _data.fillna(0)
            _data["weight"] = _data["N"] / _data["N"].sum()
            value = tsui_wang(_data, gamma=gamma)

            output.append({
                "value": value,
                "candidate": candidate,
                location_level: geography
            })

        df_dv = pd.DataFrame(output)

    elif method == "wstd":
        def weighted_sd(input_df):
            weights = input_df["value"] / input_df["value"].sum()
            vals = input_df["rate"]
            weighted_avg = np.average(vals, weights=weights)
            numer = np.sum(weights * (vals - weighted_avg)**2)
            denom = ((vals.count()-1)/vals.count())*np.sum(weights)
            return np.sqrt(numer/denom)

        _ = "rate"
        # Uses  Weighted Standard deviation of a feature to measure how divisive a location is.
        df_dv = data.groupby([location_level, "candidate"])\
            .apply(weighted_sd)\
            .reset_index()\
            .rename(columns={0: "value"})

    elif method == "nv":

        nv_output = []
        for idx, dt in data.groupby(location_level):
            geography = idx
            print(geography)

            dd = dt.groupby("candidate").agg({"value": "sum"})
            dd["rate"] = dd.apply(lambda x: x/x.sum())
            # [dd["rate"] > RATE_THRESHOLD]
            values = list(dd.sort_values("rate", ascending=False).head(
                n_candidates).index.unique())

            # if country == "United States":
            #     values = list(dd.sort_values(
            #         "rate", ascending=False).head(2).index.unique())
            #     print(values)

            dt = dt[dt["candidate"].isin(values)].copy()

            ee = dt.groupby("polling_id").agg({"value": "sum"})
            values_polling = list(
                ee[ee["value"] > VOTES_POLLING].index.unique())
            dt = dt[dt["polling_id"].isin(values_polling)].copy()

            # Re-calculates voting percentage of candidates after removing outliers
            tt = dt.groupby(["polling_id", "candidate"]).agg({"value": "sum"})
            tt["rate"] = tt.groupby(
                level=[0], group_keys=False).apply(lambda x: x/x.sum())
            tt = tt.reset_index()
            tt["rank"] = tt.groupby(["polling_id"])["value"].rank(
                "min", ascending=False).astype(int)
            dt = tt[["polling_id", "candidate", "rate", "rank", "value"]]

            dt = pd.merge(
                dt, df_location[["polling_id", location_level]]).drop_duplicates()
            dt = dt.reset_index(drop=True)

            df_between = between_p(dt)
            df_between["type"] = "Between"

            df_within = within_p(dt)
            df_within["type"] = "Within"

            df_polarization = pd.concat([df_between, df_within])

            df_polarization[location_level] = geography

            nv_output.append(df_polarization)

        df_dv = pd.concat(nv_output, ignore_index=True)

    mname = method
    if method == "er":
        mname = f"{mname}{ER_alpha}"
    elif method == "tw":
        mname = f"{mname}{TW_gamma}"

    if flag_candidates == 0:
        mname = f"{mname}_all"

    path = f"data_output/{country}/{year}_divisiveness_{location_level}_{mname}_{election_round}.csv.gz"
    df_dv.to_csv(path, compression="gzip", index=False)

    # location = "polling_id"

    # def get_rate(df, location=["polling_id"], level=[0]):
    #     df_tmp = df.groupby(location + ["candidate"]).agg({"value": "sum"})
    #     df_tmp["rate"] = df_tmp.groupby(level=level).apply(lambda x: x/x.sum())
    #     df_tmp = df_tmp.reset_index()

    #     return df_tmp

    # if regression:
    #     df = pd.merge(df, df_location, on="polling_id")

    #     df_a = get_rate(
    #         df, location=[location_level, "polling_id"], level=[0, 1])
    #     df_a = pd.merge(df_a, df_dv.rename(columns={"value": "divisiveness"}), on=[
    #                     location_level, "candidate"])

    #     if country in runoff_countries:
    #         df_b = get_rate(df_runoff)

    #     dd = df_a.pivot_table(
    #         index=["polling_id"],
    #         columns=["candidate"],
    #         values=["rate", "value", "divisiveness"]
    #     ).reset_index()

    #     cols = []
    #     for column in dd.columns:
    #         a, b = column
    #         new_column = None
    #         if not b:
    #             new_column = a
    #         else:
    #             new_column = f"{a}_{b}"  # labels

    #         cols.append(new_column)

    #     dd.columns = cols
    #     if country in runoff_countries:
    #         dd = pd.merge(dd, df_b, on="polling_id")
    #     encoding = "utf-8"
    #     if country == "Romania":
    #         encoding = "iso8859_16"
    #     dd.to_csv(
    #         f"data_regressions/{country}_{year}_polling_station_{method}.csv",
    #         encoding=encoding,
    #         index=False
    #     )


calculate_divisiveness(
    year, country, location_level, election_round, method)
